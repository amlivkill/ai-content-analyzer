import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import PyPDF2
import docx
import io
import chardet
from datetime import datetime
import json

# ==================== PAGE CONFIG (SEO Friendly) ====================
st.set_page_config(
    page_title="AI Content Analyzer - Extract Insights from Files & URLs",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/amlivkill/ai-content-analyzer',
        'Report a bug': "https://github.com/amlivkill/ai-content-analyzer/issues",
        'About': "### AI Content Analyzer\nExtract key insights from PDF, CSV, DOCX, TXT files and URLs using advanced AI technology."
    }
)

# ==================== CUSTOM CSS (Better UI) ====================
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .feature-card {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
    }
    .warning-box {
        background-color: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #ffeaa7;
    }
    .stProgress > div > div > div > div {
        background-color: #1f77b4;
    }
    .footer {
        text-align: center;
        margin-top: 3rem;
        padding: 1rem;
        color: #6c757d;
    }
</style>
""", unsafe_allow_html=True)

# ==================== SESSION STATE ====================
if 'analysis_history' not in st.session_state:
    st.session_state.analysis_history = []

# ==================== HELPER FUNCTIONS ====================
def extract_text_from_pdf(file):
    """PDF ‡§∏‡•á text extract ‡§ï‡§∞‡•á‡§Ç"""
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"PDF reading error: {str(e)}"

def extract_text_from_txt(file):
    """TXT file ‡§∏‡•á text extract ‡§ï‡§∞‡•á‡§Ç"""
    try:
        raw_data = file.read()
        encoding = chardet.detect(raw_data)['encoding']
        file.seek(0)
        return file.read().decode(encoding or 'utf-8')
    except Exception as e:
        return f"TXT reading error: {str(e)}"

def extract_data_from_csv(file):
    """CSV file ‡§∏‡•á data extract ‡§ï‡§∞‡•á‡§Ç"""
    try:
        df = pd.read_csv(file)
        return df
    except Exception as e:
        return f"CSV reading error: {str(e)}"

def scrape_website_content(url):
    """Website content scrape ‡§ï‡§∞‡•á‡§Ç"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove scripts and styles
        for element in soup(["script", "style", "nav", "footer"]):
            element.decompose()
        
        # Get main content
        title = soup.find('title')
        title_text = title.get_text() if title else "No Title"
        
        # Try to get main content
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        text = main_content.get_text(separator='\n', strip=True)
        
        return {
            'title': title_text,
            'content': text[:5000],  # Limit content
            'url': url,
            'status': 'success'
        }
    except Exception as e:
        return {'status': 'error', 'error': str(e)}

# ==================== MOCK AI ANALYSIS (OpenAI API ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ) ====================
def mock_ai_analysis(content, analysis_type):
    """OpenAI API ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ mock analysis"""
    if analysis_type == "summary":
        return f"üìù **‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂:** ‡§Ø‡§π ‡§ï‡§Ç‡§ü‡•á‡§Ç‡§ü {len(content)} characters ‡§ï‡§æ ‡§π‡•à‡•§ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§µ‡§ø‡§ö‡§æ‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§°‡•á‡§ü‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£, AI ‡§§‡§ï‡§®‡•Ä‡§ï, ‡§î‡§∞ ‡§ë‡§ü‡•ã‡§Æ‡•á‡§∂‡§® ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç‡•§"
    
    elif analysis_type == "key_points":
        return "üéØ **‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡§ø‡§Ç‡§¶‡•Å:**\n- ‡§°‡•á‡§ü‡§æ ‡§è‡§ï‡§§‡•ç‡§∞‡•Ä‡§ï‡§∞‡§£ ‡§î‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£\n- ‡§∏‡•ç‡§µ‡§ö‡§æ‡§≤‡§ø‡§§ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§ú‡§®‡§∞‡•á‡§∂‡§®\n- ‡§Æ‡§≤‡•ç‡§ü‡•Ä-‡§´‡•â‡§∞‡•ç‡§Æ‡•á‡§ü ‡§∏‡§™‡•ã‡§∞‡•ç‡§ü\n- ‡§∞‡§ø‡§Ø‡§≤-‡§ü‡§æ‡§á‡§Æ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó"
    
    elif analysis_type == "sentiment":
        return "üòä **‡§≠‡§æ‡§µ‡§®‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£:** ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï - ‡§ï‡§Ç‡§ü‡•á‡§Ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ ‡§î‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•à"

# ==================== MAIN APP ====================
def main():
    # ==================== HERO SECTION ====================
    st.markdown('<h1 class="main-header">üîç AI Content Analyzer</h1>', unsafe_allow_html=True)
    st.markdown("""
    <div style='text-align: center; margin-bottom: 3rem;'>
        <h3 style='color: #6c757d;'>PDF, CSV, DOCX, TXT Files ‡§î‡§∞ Websites ‡§∏‡•á Automatically Insights Extract ‡§ï‡§∞‡•á‡§Ç</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # ==================== FEATURES GRID ====================
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class='feature-card'>
            <h4>üìÑ Multiple Formats</h4>
            <p>PDF, CSV, DOCX, TXT ‡§∏‡§™‡•ã‡§∞‡•ç‡§ü</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class='feature-card'>
            <h4>üåê URL Analysis</h4>
            <p>‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä Website ‡§ï‡§æ Content Analyze ‡§ï‡§∞‡•á‡§Ç</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class='feature-card'>
            <h4>ü§ñ AI Powered</h4>
            <p>Smart Summarization ‡§î‡§∞ Key Points</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown("""
        <div class='feature-card'>
            <h4>üíæ Export Data</h4>
            <p>Results ‡§ï‡•ã Download ‡§ï‡§∞‡•á‡§Ç</p>
        </div>
        """, unsafe_allow_html=True)
    
    # ==================== SIDEBAR ====================
    with st.sidebar:
        st.image("https://streamlit.io/images/brand/streamlit-mark-color.png", width=50)
        st.title("Settings")
        
        # API Configuration
        st.subheader("API Configuration")
        api_key = st.text_input("OpenAI API Key", type="password", 
                               help="Get your API key from https://platform.openai.com/")
        
        # Analysis Options
        st.subheader("Analysis Options")
        analysis_types = st.multiselect(
            "Select Analysis Types",
            ["Summary", "Key Points", "Sentiment Analysis", "Word Frequency"],
            default=["Summary", "Key Points"]
        )
        
        # File Size Limit
        max_file_size = st.slider("Max File Size (MB)", 1, 50, 10)
        
        st.markdown("---")
        st.markdown("""
        **How to use:**
        1. File upload ‡§ï‡§∞‡•á‡§Ç ‡§Ø‡§æ URL ‡§°‡§æ‡§≤‡•á‡§Ç
        2. Analysis options select ‡§ï‡§∞‡•á‡§Ç
        3. Analyze ‡§¨‡§ü‡§® ‡§™‡§∞ ‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ï‡§∞‡•á‡§Ç
        4. Results download ‡§ï‡§∞‡•á‡§Ç
        """)
    
    # ==================== MAIN CONTENT AREA ====================
    tab1, tab2, tab3 = st.tabs(["üìÅ File Upload", "üåê URL Analysis", "üìä Analysis History"])
    
    with tab1:
        st.header("File Upload & Analysis")
        
        uploaded_files = st.file_uploader(
            "Choose files to analyze",
            type=['pdf', 'csv', 'txt', 'docx'],
            accept_multiple_files=True,
            help="Supported formats: PDF, CSV, TXT, DOCX"
        )
        
        if uploaded_files:
            st.markdown(f'<div class="success-box">‚úÖ {len(uploaded_files)} file(s) selected</div>', unsafe_allow_html=True)
            
            for file in uploaded_files:
                with st.expander(f"üìÑ {file.name}", expanded=True):
                    col1, col2 = st.columns([1, 3])
                    
                    with col1:
                        st.write(f"**Type:** {file.type}")
                        st.write(f"**Size:** {file.size / 1024:.1f} KB")
                    
                    with col2:
                        if st.button(f"Analyze {file.name}", key=f"btn_{file.name}"):
                            with st.spinner("Analyzing content..."):
                                # File processing based on type
                                if file.type == "application/pdf":
                                    content = extract_text_from_pdf(file)
                                elif file.type == "text/plain":
                                    content = extract_text_from_txt(file)
                                elif file.type == "text/csv":
                                    content = extract_data_from_csv(file)
                                else:
                                    content = "File type processing coming soon..."
                                
                                # Display results
                                if "Summary" in analysis_types:
                                    st.subheader("üìù Summary")
                                    st.write(mock_ai_analysis(content, "summary"))
                                
                                if "Key Points" in analysis_types:
                                    st.subheader("üéØ Key Points")
                                    st.write(mock_ai_analysis(content, "key_points"))
                                
                                # Save to history
                                st.session_state.analysis_history.append({
                                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                    'type': 'file',
                                    'name': file.name,
                                    'analysis': 'completed'
                                })
    
    with tab2:
        st.header("Website URL Analysis")
        
        url_input = st.text_input(
            "Enter website URL",
            placeholder="https://example.com",
            help="Enter full URL including http:// or https://"
        )
        
        col1, col2 = st.columns([3, 1])
        with col1:
            if url_input:
                if st.button("üåê Analyze Website", type="primary"):
                    with st.spinner("Scraping and analyzing website content..."):
                        website_data = scrape_website_content(url_input)
                        
                        if website_data['status'] == 'success':
                            st.markdown('<div class="success-box">‚úÖ Website content extracted successfully!</div>', unsafe_allow_html=True)
                            
                            st.subheader("üåê Website Information")
                            st.write(f"**Title:** {website_data['title']}")
                            st.write(f"**URL:** {website_data['url']}")
                            st.write(f"**Content Length:** {len(website_data['content'])} characters")
                            
                            # Analysis results
                            if "Summary" in analysis_types:
                                st.subheader("üìù Summary")
                                st.write(mock_ai_analysis(website_data['content'], "summary"))
                            
                            if "Key Points" in analysis_types:
                                st.subheader("üéØ Key Points")
                                st.write(mock_ai_analysis(website_data['content'], "key_points"))
                            
                            # Save to history
                            st.session_state.analysis_history.append({
                                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                'type': 'url',
                                'name': url_input,
                                'analysis': 'completed'
                            })
                        else:
                            st.error(f"Error: {website_data['error']}")
    
    with tab3:
        st.header("Analysis History")
        
        if st.session_state.analysis_history:
            for i, history in enumerate(reversed(st.session_state.analysis_history[-10:])):  # Last 10 entries
                with st.expander(f"{history['timestamp']} - {history['name']}"):
                    st.write(f"**Type:** {history['type']}")
                    st.write(f"**Status:** {history['analysis']}")
        else:
            st.info("No analysis history yet. Upload files or analyze URLs to see history here.")
    
    # ==================== FOOTER (SEO Friendly) ====================
    st.markdown("---")
    st.markdown("""
    <div class='footer'>
        <p><strong>AI Content Analyzer</strong> - Extract meaningful insights from your documents and websites</p>
        <p>Built with ‚ù§Ô∏è using Streamlit | Support: amlivkill@github</p>
    </div>
    """, unsafe_allow_html=True)

# ==================== RUN APP ====================
if __name__ == "__main__":
    main()
